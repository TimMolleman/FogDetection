{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torchsample\n",
    "import psycopg2\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "import csv\n",
    "import copy\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data_utils\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "from torchsample import transforms as ts_transforms\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from scipy.ndimage import imread\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from mlxtend.evaluate import confusion_matrix\n",
    "from mlxtend.plotting import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Data Highway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = np.load('/Volumes/TIMPP/UnusedKNMI/numpyfiles/meteo/highway/split/highway_train_images.npy')\n",
    "train_targets = np.load('/Volumes/TIMPP/UnusedKNMI/numpyfiles/meteo/highway/split/highway_train_targets.npy')\n",
    "paths_train = np.load('/Volumes/TIMPP/UnusedKNMI/numpyfiles/meteo/highway/split/highway_train_filepaths.npy')\n",
    "meteo_train = np.load('/Volumes/TIMPP/UnusedKNMI/numpyfiles/meteo/highway/split/highway_train_meteo.npy')\n",
    "meteo_train = meteo_train.reshape(meteo_train.shape[0], 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('/Volumes/TIMPP/UnusedKNMI/numpyfiles/meteo/knmi/knmi_train_images.npy')\n",
    "train_targets = np.load('/Volumes/TIMPP/UnusedKNMI/numpyfiles/meteo/knmi/knmi_train_targets.npy')\n",
    "paths_train = np.load('/Volumes/TIMPP/UnusedKNMI/numpyfiles/meteo/knmi/knmi_train_filepaths.npy')\n",
    "meteo_train = np.load('/Volumes/TIMPP/UnusedKNMI/numpyfiles/meteo/knmi/knmi_train_meteo.npy')\n",
    "meteo_train = meteo_train.reshape(meteo_train.shape[0], 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_validation = np.load('/Volumes/TIMPP/UnusedKNMI/numpyfiles/meteo/highway/split/highway_val_images.npy')\n",
    "validation_targets = np.load('/Volumes/TIMPP/UnusedKNMI/numpyfiles/meteo/highway/split/highway_val_targets.npy')\n",
    "paths_validation = np.load('/Volumes/TIMPP/UnusedKNMI/numpyfiles/meteo/highway/split/highway_val_filepaths.npy')\n",
    "meteo_validation = np.load('/Volumes/TIMPP/UnusedKNMI/numpyfiles/meteo/highway/split/highway_val_meteo.npy')\n",
    "meteo_validation = meteo_validation.reshape(meteo_validation.shape[0], 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_features = np.load('/Volumes/TIMPP/UnusedKNMI/numpyfiles/test_images.npy')\n",
    "test_targets = np.load('/Volumes/TIMPP/UnusedKNMI/numpyfiles/test_targets.npy')\n",
    "test_filepaths = np.load('/Volumes/TIMPP/UnusedKNMI/numpyfiles/test_filepaths.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[:3000]\n",
    "train_targets = train_targets[:3000]\n",
    "paths_train = paths_train[:3000]\n",
    "meteo_train = meteo_train[:3000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Helpers and Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Proportions (for Weighted Sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class percentages:\n",
      "No fog: 98.95%\n",
      "Fog: 0.49%\n",
      "Dense fog: 0.55%\n",
      "[121982    607    684]\n"
     ]
    }
   ],
   "source": [
    "# Get the class proportions\n",
    "class_counts = np.bincount(train_targets.astype(int))\n",
    "total = len(train_targets)\n",
    "proportion_0 = class_counts[0] / total\n",
    "proportion_1 = class_counts[1] / total\n",
    "proportion_2 = class_counts[2] / total\n",
    "\n",
    "print('Class percentages:\\nNo fog: {:.2f}%\\nFog: {:.2f}%\\nDense fog: {:.2f}%'.format(proportion_0 * 100,\n",
    "                                                                              proportion_1 * 100, proportion_2 * 100))\n",
    "print(class_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List containing class probabilities\n",
    "probabilities = [proportion_0, proportion_1, proportion_2]\n",
    "reciprocal_weights = []\n",
    "\n",
    "# Put weight at every index\n",
    "for i in range(len(X_train)):\n",
    "    reciprocal_weights.append(probabilities[train_targets[i]])\n",
    "\n",
    "# Inverse of probabilities as weights\n",
    "weights = (1 / torch.Tensor(reciprocal_weights))\n",
    "sampler = torch.utils.data.sampler.WeightedRandomSampler(weights.double(), len(X_train))\n",
    "\n",
    "# Inverse weights for all the datapoints\n",
    "inverse_weights_class = 1 / torch.Tensor(probabilities)\n",
    "\n",
    "# Inverse weights per class\n",
    "inverse_weights = 1/ torch.Tensor(probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.RandomCrop(80),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'validation': transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.CenterCrop(80),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class KNMIDataset(Dataset):\n",
    "    def __init__(self, images, targets, filepaths, meteo, transforms=None):\n",
    "    \n",
    "        self.transforms = transforms\n",
    "        self.images = images\n",
    "        self.targets = targets\n",
    "        self.filepaths = filepaths\n",
    "        self.meteo = meteo\n",
    "        print(len(self.images))\n",
    "    def __getitem__(self, index):\n",
    "        image = self.images[index]\n",
    "        \n",
    "        if self.transforms != None:\n",
    "            image = self.transforms(image)\n",
    "        \n",
    "        target = self.targets[index]\n",
    "        filepath = self.filepaths[index]\n",
    "        meteo = self.meteo[index]\n",
    "        \n",
    "        return (image, target, index, filepath, meteo)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Datasets and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 164\n",
    "\n",
    "# Datasets\n",
    "train_dataset = KNMIDataset(X_train, train_targets, paths_train, meteo_train, transforms=data_transforms['train'])\n",
    "validation_dataset = KNMIDataset(X_validation, validation_targets, paths_validation, meteo_validation, transforms=data_transforms['validation'])\n",
    "\n",
    "# Data loaders\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "validation_loader = DataLoader(dataset=validation_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "loaders = {'train': train_loader, 'validation': validation_loader}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Batch Iteration Size of Trainloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration for one train/testloader batch\n",
    "img, labels, idx, paths, meteo = next(iter(validation_loader))\n",
    "inputs, labels = Variable(img), Variable(labels)\n",
    "print('Loader image tensor shape: {}\\nLoader targets tensor shape: {}'.format(inputs.size(), labels.size()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " # Confusion matrix helper\n",
    "def show_cm(targets, predictions):\n",
    "    cm = confusion_matrix(y_target=targets, \n",
    "                      y_predicted=predictions, \n",
    "                      binary=False)\n",
    "\n",
    "    fig, ax = plot_confusion_matrix(conf_mat=cm)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Curve Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_loss_curves(training_loss, validation_loss):\n",
    "    \"\"\"\n",
    "    Plots loss curves after model training.\n",
    "    \n",
    "    :param training_loss: List with training loss for every epoch.\n",
    "    :param validation_loss: List with validation loss for every epoch.\n",
    "    \"\"\"\n",
    "    train_plot, = plt.plot(training_loss, label='Training')\n",
    "    val_plot, = plt.plot(validation_loss, label='Validation')\n",
    "    plt.title('Loss curves (training/validation)')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(handles=[train_plot, val_plot])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Certain/Uncertain Images Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_images(loss, image_index, filepaths, targets, predictions, phase, amount=5):\n",
    "    \"\"\"\n",
    "    Use to plot images that the model is most certain about and which it was most uncertain about.\n",
    "    \n",
    "    :param loss: Tensor that has size of batch containing loss\n",
    "    :param filepaths: List with filepaths that point to where batch images are located\n",
    "    :param amount: Amount of images to show. Default: 1\n",
    "    \"\"\"\n",
    "    \n",
    "    def loop_plot(indices, targets, predictions, filepaths, losses, phase):    \n",
    "        fig=plt.figure(figsize=(20, 5))\n",
    "        columns = 5\n",
    "        rows = 1\n",
    "        \n",
    "        # Determine phase and get image np array\n",
    "        if phase == 'train':\n",
    "            image_array = X_train\n",
    "        elif phase == 'validation':\n",
    "            image_array = X_validation\n",
    "        else:\n",
    "            image_array = test_images\n",
    "        print(filepaths)\n",
    "        \n",
    "        # Loop over the data and plot 'amount' of images\n",
    "        for i, (index, target, prediction, loss) in enumerate(zip(indices, targets, predictions, losses)):\n",
    "            img = image_array[index]\n",
    "            fig.add_subplot(rows, columns, i + 1)\n",
    "            plt.title('target: {}, prediction: {} loss: {:.2f}'.format(target, prediction, loss))\n",
    "            plt.imshow(img)\n",
    "      \n",
    "        plt.show()\n",
    "    \n",
    "    def get_k_and_plot(loss, amount, targets, image_index, filepaths, predictions, phase, largest=True):\n",
    "        # Get all relevant data\n",
    "        values, indices = torch.topk(loss, amount, largest=largest)\n",
    "        targets = [targets[i].data[0] for i in list(indices.data.numpy().reshape((1, -1))[0])]\n",
    "        images_idx = [image_index[i] for i in list(indices.data.numpy().reshape((1, -1))[0])]\n",
    "        filepaths = [filepaths[i] for i in list(indices.data.numpy().reshape((1, -1))[0])]\n",
    "        predictions = [predictions[i] for i in list(indices.data.numpy().reshape((1, -1))[0])]\n",
    "        loss = [loss.data[i] for i in list(indices.data.numpy().reshape((1, -1))[0])]\n",
    "\n",
    "        # Show images (uncertain/certain)\n",
    "        loop_plot(images_idx, targets, predictions, filepaths, loss, phase)\n",
    "\n",
    "    print('Top {} most uncertain images'.format(amount))\n",
    "    get_k_and_plot(loss, amount, targets, image_index, filepaths, predictions, phase, largest=True)\n",
    "\n",
    "    print('Top {} most certain images'.format(amount))\n",
    "    get_k_and_plot(loss, amount, targets, image_index, filepaths, predictions, phase, largest=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_accuracy(predictions, targets):\n",
    "  \n",
    "    # Lists for holding corrects\n",
    "    no_fog_correct = 0\n",
    "    light_fog_correct = 0\n",
    "    dense_fog_correct = 0\n",
    "\n",
    "    for pred, target in zip(predictions, targets):\n",
    "        if pred == 0 and target == 0:\n",
    "            no_fog_correct += 1\n",
    "        elif pred == 1 and target == 1:\n",
    "            light_fog_correct += 1\n",
    "        elif pred == 2 and target == 2:\n",
    "            dense_fog_correct += 1\n",
    "\n",
    "    # Validation counts\n",
    "    total = np.bincount(validation_targets)\n",
    "    no_fog_total = total[0]\n",
    "    light_fog_total = total[1]\n",
    "    dense_fog_total = total[2]\n",
    "\n",
    "    # Accuracy per class\n",
    "    acc_no_fog = no_fog_correct / no_fog_total\n",
    "    acc_light = light_fog_correct / light_fog_total\n",
    "    acc_dense = dense_fog_correct / dense_fog_total\n",
    "\n",
    "    average_acc = (acc_no_fog + acc_light + acc_dense) / 3 * 100\n",
    "\n",
    "    return average_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, num_epochs):\n",
    "    \"\"\"\n",
    "    Does the actual training of the models.\n",
    "    \n",
    "    :param model: Model object specified in 'run_model'.\n",
    "    :param criterion: Optimization criterion/loss.\n",
    "    :param optimizer: Type of optimizer.\n",
    "    :param num_epochs: Number of epochs to train.\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "\n",
    "    # For storing loss for curve, best model and best accuracy\n",
    "    train_loss, validation_loss = [],[]\n",
    "    best_model = model\n",
    "    best_accuracy = 0.0\n",
    "    best_f1macro = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        # Running loss and correct predictions\n",
    "        running_loss_train = 0.0\n",
    "        running_correct_train = 0.0\n",
    "        running_loss_val = 0.0\n",
    "        running_correct_val = 0.0\n",
    "        epoch_validation_targets = []\n",
    "        epoch_validation_predictions = []\n",
    "        \n",
    "\n",
    "        for phase in ['train', 'validation']:\n",
    "\n",
    "            # Change model mode according to phase \n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            # Iterate over batches in loader\n",
    "            for i, (image_tensor, label_tensor, image_index, filepaths, meteo) in enumerate(loaders[phase]):\n",
    "\n",
    "                features = Variable(image_tensor)\n",
    "                targets = Variable(label_tensor.view(-1))\n",
    "                meteo_features = Variable(meteo.type(torch.FloatTensor))\n",
    "                \n",
    "\n",
    "                # Forward + Backward + Optimize\n",
    "                if phase == 'train':\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                outputs = model(features, meteo_features)\n",
    "\n",
    "                # Get prediction index and no. correct predictions\n",
    "                _, predictions = torch.max(outputs.data, 1) \n",
    "                \n",
    "                # Kijk hier uit dat die testloader precies aantal batches in de validation data haalt \n",
    "                correct = torch.sum(predictions == targets.data) \n",
    "\n",
    "                # Loss and optimization\n",
    "                loss = criterion(outputs, targets)\n",
    "                \n",
    "                # Average the loss\n",
    "                total_loss = torch.mean(loss)\n",
    "                \n",
    "                # Only do backpropagation if in the training phase\n",
    "                if phase == 'train':\n",
    "                    total_loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                # Running loss and number of correct predictions\n",
    "                if phase == 'train':\n",
    "                    running_loss_train += total_loss.data[0]\n",
    "                    running_correct_train += correct\n",
    "                else:\n",
    "                    running_loss_val += total_loss.data[0]\n",
    "                    running_correct_val += correct\n",
    "                    epoch_validation_targets.extend(list(targets.data))\n",
    "                    epoch_validation_predictions.extend(list(predictions))\n",
    "                    \n",
    "                    # Plot images in validation phase\n",
    "#                     plot_images(loss, image_index, filepaths, targets, predictions, phase)\n",
    "\n",
    "                # If model is in training phase, show loss every N iterations\n",
    "                if (i+1) % 5 == 0:\n",
    "                    if phase == 'train':\n",
    "                        print ('Epoch {}/{}, Iteration {}/{} Train Running Loss: {:.4f}'.format(epoch+1, num_epochs, i+1, \n",
    "                                                                                    len(X_train)//BATCH_SIZE, \n",
    "                                                                                    running_loss_train / i))\n",
    "#                         plot_images(loss, image_index, filepaths, targets, predictions, phase)\n",
    "                    \n",
    "\n",
    "        # Epoch losses and epoch train accuracies\n",
    "        epoch_train_loss = running_loss_train / (len(X_train)//BATCH_SIZE)\n",
    "        epoch_train_accuracy = (running_correct_train / (len(X_train)//BATCH_SIZE)) / BATCH_SIZE * 100\n",
    "        epoch_val_loss = running_loss_val / len(X_validation//BATCH_SIZE)\n",
    "        epoch_val_accuracy= running_correct_val / len(X_validation) * 100\n",
    "        \n",
    "        # F1-score\n",
    "        f1_macro = f1_score(epoch_validation_targets, epoch_validation_predictions, average='macro')\n",
    "        f1_micro = f1_score(epoch_validation_targets, epoch_validation_predictions, average='micro')\n",
    "        precision = precision_score(epoch_validation_targets, epoch_validation_predictions, average='macro')\n",
    "        recall = recall_score(epoch_validation_targets, epoch_validation_predictions, average='macro')\n",
    "\n",
    "        # Print the average epoch loss and the average prediction accuracy\n",
    "        print('\\nEpoch {}/{}, Train Loss: {:.4f}, Train Accuracy: {:.4f}%\\n'\n",
    "              'Validation Loss: {:.4f}, Validation Accuracy: {:.4f}%, f1-score {:.4f}\\n'.format(epoch + 1, \n",
    "                                                                    num_epochs, epoch_train_loss, epoch_train_accuracy,\n",
    "                                                                     epoch_val_loss, epoch_val_accuracy, f1_macro))\n",
    "        \n",
    "        # Safe best model and best accuracy \n",
    "        if phase == 'validation':\n",
    "            if epoch_val_accuracy > best_accuracy:\n",
    "                best_accuracy = epoch_val_accuracy\n",
    "                best_model = copy.deepcopy(model)\n",
    "\n",
    "        # Show the confusion matrix for validation targets/predictions\n",
    "        show_cm(epoch_validation_targets, epoch_validation_predictions)\n",
    "        \n",
    "        # Append losses\n",
    "        train_loss.append(epoch_train_loss)\n",
    "        validation_loss.append(epoch_val_loss)\n",
    "        \n",
    "    # Elapsed time and best accuracy\n",
    "    elapsed_time = time.time() - start\n",
    "    print('Training was completed in {:.0f}m {:.0f}s\\n'.format(elapsed_time//60, elapsed_time%60))\n",
    "    print('Best validation accuracy: {:4f}%'.format(best_accuracy))\n",
    "    \n",
    "    # Plot loss curves\n",
    "#     plot_loss_curves(train_loss, validation_loss)\n",
    "    \n",
    "    # Return the best model\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Model Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_model(model, epochs, learning_rate, train_from_layer=False, last_layer_trained=False, not_self_defined=True):\n",
    "    '''\n",
    "    Configures model object and then calls 'train_model' for model training. \n",
    "    \n",
    "    :param train_from_layer: Specify number of layers before fully-connected to also be finetuned. Default is False,\n",
    "    which will just train the fc layers. Give a number to specify number of layers before that. \n",
    "    :param model: This is a (pre-trained) model that will be further finetuned.\n",
    "    :param epochs: Number of epochs to train.\n",
    "    :param learning_rate: Learning rate for the parameters.\n",
    "    :param not_self_defined: True if model architecture is used from torchvision. False if model is self-defined. \n",
    "    '''\n",
    "    \n",
    "    if not_self_defined:\n",
    "      \n",
    "        # Set all parameter training to false\n",
    "        for parameter in model.parameters():\n",
    "            parameter.requires_grad = False\n",
    "\n",
    "        # Select number of pre-trained layers to finetune\n",
    "        if train_from_layer != False:\n",
    "            ct = 0\n",
    "            print(ct)\n",
    "\n",
    "            for name, child in model.named_children():\n",
    "                ct += 1\n",
    "\n",
    "                if ct > train_from_layer:\n",
    "                    for name2, params in child.named_parameters():\n",
    "                        params.requires_grad = True\n",
    "\n",
    "          # Get parameters that need finetuning\n",
    "            optim_params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "\n",
    "    # Adjust final layer to number of classes if last layer has not been trained yet\n",
    "    if last_layer_trained == False:\n",
    "        num_features = model.fc.in_features\n",
    "        model.fc = nn.Linear(num_features, TARGET_SIZE)\n",
    "        optim_params = model.fc.parameters()\n",
    "    \n",
    "    # Train all parameters if model is not predefined from torchvision\n",
    "    else: \n",
    "        optim_params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        inverse_weights.cuda()\n",
    "        model = model.cuda()\n",
    "    \n",
    "    # Optimizers and loss criterions\n",
    "    criterion = nn.CrossEntropyLoss(reduce=False, weight=inverse_weights)\n",
    "    optimizer = optim.Adam(optim_params, lr=learning_rate)        \n",
    "    \n",
    "    # Train and save\n",
    "    trained_model = train_model(model, criterion, optimizer, epochs)\n",
    "    \n",
    "    return trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 4\n",
    "hidden_size = 100\n",
    "output_size = 3\n",
    "\n",
    "class meteo_NN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
    "        super(meteo_NN, self).__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Input layer\n",
    "        self.fc_input = nn.Linear(input_size, hidden_size)  \n",
    "\n",
    "        # Hidden size dependent on input\n",
    "        if num_layers == 1:      \n",
    "            self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "            self.fc_last = nn.Linear(hidden_size, output_size)\n",
    "            \n",
    "        elif num_layers == 2:\n",
    "            self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "            self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "            self.fc_last = nn.Linear(hidden_size, output_size)\n",
    "            \n",
    "        elif num_layers == 3:\n",
    "            self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "            self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "            self.fc4 = nn.Linear(hidden_size, hidden_size)\n",
    "            self.fc_last = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        else:\n",
    "            raise ValueError('This net only takes a maximum of 3 hidden layers as input.')\n",
    "                             \n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.fc_input(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        if self.num_layers == 1:\n",
    "            x = self.fc2(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.fc_last(x)\n",
    "            \n",
    "        elif self.num_layers == 2:\n",
    "            x = self.fc2(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.fc3(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.fc_last(x)\n",
    "        else:\n",
    "            x = self.fc2(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.fc3(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.fc4(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.fc_last(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class resnet18_meteo(nn.Module):\n",
    "    def __init__(self, resnet18, meteo_NN, num_classes):\n",
    "        super(resnet18_meteo, self).__init__()\n",
    "        \n",
    "        # Respectively a torchvision resnet-18 and a 1-hidden layer NN\n",
    "        self.resnet_CNN = resnet18\n",
    "        self.meteo_net = meteo_NN\n",
    "        \n",
    "        # Sizes of the FC layers of both NN's\n",
    "        self.len_fc_resnet = self.resnet_CNN.fc.in_features\n",
    "        self.len_fc_meteo = self.meteo_net.fc_last.out_features\n",
    "        print(self.len_fc_meteo)\n",
    "        \n",
    "        # Remove FC layer from the resnet \n",
    "        self.modules=list(self.resnet_CNN.children())[:-1]\n",
    "        self.resnet18_convblocks= nn.Sequential(*self.modules)\n",
    "        \n",
    "        # Fully connected layer is now size resnet FC + meteo FC\n",
    "        self.fc = nn.Linear(self.len_fc_resnet + self.len_fc_meteo, num_classes)\n",
    "    \n",
    "    def forward(self, img_x, meteo_x):\n",
    "        \n",
    "        # Both should be flattened layers at end of networks\n",
    "        img_x = self.resnet18_convblocks(img_x)\n",
    "        meteo_x = self.meteo_net(meteo_x)\n",
    "        \n",
    "        # Flatten convolutional features\n",
    "        img_x_flattened = img_x.view(img_x.size(0), -1)\n",
    "        \n",
    "        # Concat the outputs of CNN and meteo-NN in fully connected layer\n",
    "        out = torch.cat([img_x_flattened, meteo_x], dim=1)\n",
    "        out = self.fc(out)\n",
    "        return out   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define meteo feedforward net\n",
    "meteo_net = meteo_NN(input_size, hidden_size, output_size, 2)\n",
    "\n",
    "# Define Resnet and remove FC layer\n",
    "resnet18 = models.resnet18(pretrained=True)\n",
    "resnet18.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "# Meteo + resnet18\n",
    "meteo_resnet = resnet18_meteo(resnet18, meteo_net, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_FFN(model, criterion, optimizer, num_epochs):\n",
    "    \"\"\"\n",
    "    Does the actual training of the models.\n",
    "    \n",
    "    :param model: Model object specified in 'run_model'.\n",
    "    :param criterion: Optimization criterion/loss.\n",
    "    :param optimizer: Type of optimizer.\n",
    "    :param num_epochs: Number of epochs to train.\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "\n",
    "    # For storing loss for curve, best model and best accuracy\n",
    "    train_loss, validation_loss = [],[]\n",
    "    best_accuracy = 0.0\n",
    "    best_avg_accuracy = 0.0\n",
    "    best_f1macro = 0.0\n",
    "    best_epoch_avg = 0\n",
    "    best_epoch_f1 = 0\n",
    "    best_model_avg = model\n",
    "    best_model_f1 = model\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        # Running loss and correct predictions\n",
    "        running_loss_train = 0.0\n",
    "        running_correct_train = 0.0\n",
    "        running_loss_val = 0.0\n",
    "        running_correct_val = 0.0\n",
    "        epoch_validation_targets = []\n",
    "        epoch_validation_predictions = []\n",
    "        \n",
    "        validation_predictions = []\n",
    "        validation_targets = []\n",
    "        \n",
    "        for phase in ['train', 'validation']:\n",
    "\n",
    "            # Change model mode according to phase \n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            # Iterate over batches in loader\n",
    "            for i, (image_tensor, label_tensor, image_index, filepaths, meteo) in enumerate(loaders[phase]):\n",
    "\n",
    "                features = Variable(meteo.type(torch.FloatTensor))\n",
    "                targets = Variable(label_tensor.view(-1))\n",
    "                \n",
    "\n",
    "                # Forward + Backward + Optimize\n",
    "                if phase == 'train':\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                outputs = model(features)\n",
    "\n",
    "                # Get prediction index and no. correct predictions\n",
    "                _, predictions = torch.max(outputs.data, 1) \n",
    "                \n",
    "                # Kijk hier uit dat die testloader precies aantal batches in de validation data haalt \n",
    "                correct = torch.sum(predictions == targets.data) \n",
    "                \n",
    "                if phase == 'validation':\n",
    "                    validation_predictions.extend(predictions.cpu().numpy())\n",
    "                    validation_targets.extend(targets.data.cpu().numpy())\n",
    "\n",
    "                # Loss and optimization\n",
    "                loss = criterion(outputs, targets)\n",
    "                \n",
    "                # Average the loss\n",
    "                total_loss = torch.mean(loss)\n",
    "                \n",
    "                # Only do backpropagation if in the training phase\n",
    "                if phase == 'train':\n",
    "                    total_loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                # Running loss and number of correct predictions\n",
    "                if phase == 'train':\n",
    "                    running_loss_train += total_loss.data[0]\n",
    "                    running_correct_train += correct\n",
    "                else:\n",
    "                    running_loss_val += total_loss.data[0]\n",
    "                    running_correct_val += correct\n",
    "                    epoch_validation_targets.extend(list(targets.data))\n",
    "                    epoch_validation_predictions.extend(list(predictions))\n",
    "                    \n",
    "                    # Plot images in validation phase\n",
    "#                     plot_images(loss, image_index, filepaths, targets, predictions, phase)\n",
    "\n",
    "                # If model is in training phase, show loss every N iterations\n",
    "                if (i+1) % 50 == 0:\n",
    "                    if phase == 'train':\n",
    "                        print ('Epoch {}/{}, Iteration {}/{} Train Running Loss: {:.4f}'.format(epoch+1, num_epochs, i+1, \n",
    "                                                                                    len(X_train)//BATCH_SIZE, \n",
    "                                                                                    running_loss_train / i))\n",
    "#                         plot_images(loss, image_index, filepaths, targets, predictions, phase)\n",
    "                    \n",
    "\n",
    "        # Epoch losses and epoch train accuracies\n",
    "        epoch_train_loss = running_loss_train / (len(X_train)//BATCH_SIZE)\n",
    "        epoch_train_accuracy = (running_correct_train / (len(X_train)//BATCH_SIZE)) / BATCH_SIZE * 100\n",
    "        epoch_val_loss = running_loss_val / len(X_validation//BATCH_SIZE)\n",
    "        epoch_val_accuracy= running_correct_val / len(X_validation) * 100\n",
    "        \n",
    "        # F1-score\n",
    "        f1_macro = f1_score(epoch_validation_targets, epoch_validation_predictions, average='macro')\n",
    "        f1_micro = f1_score(epoch_validation_targets, epoch_validation_predictions, average='micro')\n",
    "        \n",
    "        average_accuracy = get_average_accuracy(validation_predictions, validation_targets)\n",
    "        \n",
    "        # Print the average epoch loss and the average prediction accuracy\n",
    "        print('\\nEpoch {}/{}, Train Loss: {:.4f}, Train Accuracy: {:.4f}%\\n'\n",
    "              'Validation Loss: {:.4f}, Validation Overall Accuracy: {:.4f}%, Validation avg acc: {:4f}%, f1-score {:.4f}\\n'.format(epoch + 1, \n",
    "                                                                    num_epochs, epoch_train_loss, epoch_train_accuracy,\n",
    "                                                                     epoch_val_loss, epoch_val_accuracy, average_accuracy, f1_macro))\n",
    "        \n",
    "        # Safe best model and best accuracy \n",
    "        if phase == 'validation':\n",
    "            if epoch_val_accuracy > best_accuracy:\n",
    "                best_accuracy = epoch_val_accuracy\n",
    "                best_model = copy.deepcopy(model)\n",
    "            \n",
    "            if f1_macro > best_f1macro:\n",
    "                best_f1macro = f1_macro\n",
    "                best_epoch_f1 = epoch\n",
    "                best_model_f1 = copy.deepcopy(model)\n",
    "            \n",
    "            if average_accuracy > best_avg_accuracy:\n",
    "                best_avg_accuracy = average_accuracy\n",
    "                best_epoch_avg = epoch\n",
    "                best_model_avg = copy.deepcopy(model)\n",
    "\n",
    "        # Show the confusion matrix for validation targets/predictions\n",
    "        show_cm(epoch_validation_targets, epoch_validation_predictions)\n",
    "        \n",
    "        # Append losses\n",
    "        train_loss.append(epoch_train_loss)\n",
    "        validation_loss.append(epoch_val_loss)\n",
    "        \n",
    "        checkpoint = {\n",
    "          'epoch': epoch + 1,\n",
    "          'best_epoch_f1': best_epoch_f1,\n",
    "          'best_epoch_avg': best_epoch_avg,\n",
    "          'state_dict': model,\n",
    "          'best_f1': best_f1macro,\n",
    "          'best_accuracy': best_accuracy,\n",
    "          'optimizer': optimizer.state_dict(),\n",
    "          'best_model_avg': best_model_avg,\n",
    "          'best_model_f1': best_model_f1,\n",
    "          'validation_loss' : validation_loss,\n",
    "          'train_loss' : train_loss,\n",
    "          'best_avg_acc': best_avg_accuracy          \n",
    "        }\n",
    "        \n",
    "    # Elapsed time and best accuracy\n",
    "    elapsed_time = time.time() - start\n",
    "    print('Training was completed in {:.0f}m {:.0f}s\\n'.format(elapsed_time//60, elapsed_time%60))\n",
    "    print('Best validation accuracy: {:4f}%'.format(best_accuracy))\n",
    "    print('Best validation f1-macro: {:.4f}'.format(f1_macro))\n",
    "    print('Best average validation accuracy: {:.4f}%'.format(best_avg_accuracy))\n",
    "    # Return the best model\n",
    "    return checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meteo_input = 4\n",
    "meteo_epochs = 1\n",
    "\n",
    "#\n",
    "accuracy_dict = {}\n",
    "\n",
    "# Hyperparameters meteorological net tuning\n",
    "lr_meteo = 0.05\n",
    "num_nodes = [i for i in range(2, 12, 2)]\n",
    "num_layers = [1,2,3]\n",
    "meteo_num_epochs = 20\n",
    "\n",
    "for layer_size in num_layers:\n",
    "    for node_size in num_nodes:\n",
    "    \n",
    "        model = meteo_NN(input_size=meteo_input, hidden_size=node_size, output_size=3, num_layers=layer_size)\n",
    "        optim_params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "        \n",
    "        # Optimizers and loss criterions\n",
    "        criterion = nn.CrossEntropyLoss(reduce=False, weight=inverse_weights)\n",
    "        optimizer = optim.Adam(optim_params, lr=lr_meteo)        \n",
    "\n",
    "        trained = train_FFN(model, criterion, optimizer, meteo_epochs)\n",
    "\n",
    "        if str(layer_size) in accuracy_dict.keys():\n",
    "            accuracy_dict[str(layer_size)][str(node_size)] = {'f1' : trained['best_f1'], \n",
    "                                                                'avg_acc': trained['best_avg_acc']}\n",
    "        else:\n",
    "            accuracy_dict[str(layer_size)] = {str(node_size) : {'f1' : trained['best_f1'], \n",
    "                                                                'avg_acc': trained['best_avg_acc']}}\n",
    "        print(accuracy_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_f1, best_avg_acc, nodes_f1, layers_f1, nodes_avg, layers_avg  = 0, 0, 0, 0, 0, 0\n",
    "\n",
    "for layer_size, node_size in accuracy_dict.items():\n",
    "    for key, metrics in node_size.items():\n",
    "        if metrics['f1'] > best_f1:\n",
    "            best_f1 = metrics['f1']\n",
    "            nodes_f1 = key\n",
    "            layers_f1 = layer_size \n",
    "        if metrics['avg_acc'] > best_avg_acc:\n",
    "            best_avg_acc = metrics['avg_acc']\n",
    "            nodes_avg = key\n",
    "            layers_avg = layer_size \n",
    "\n",
    "print('Best f1 with {} layers and {} nodes. F1-macro: {}'.format(layers_f1, nodes_f1, best_f1))\n",
    "print('Best average accuracy with {} layers and {} nodes. Avg accuracy: {}'.format(layers_f1, nodes_f1, best_avg_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resnet 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_classifier = 2e-4\n",
    "lr_tuner = 1e-4\n",
    "\n",
    "# Train last FC layer of resnet 18\n",
    "resnet_18 = models.resnet18(pretrained=True)\n",
    "resnet_18.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "resnet_18_trained = run_model(resnet_18, 10, lr_classifier, False)\n",
    "\n",
    "# Tune also the last convolutional layer\n",
    "# resnet_18_tuned = run_model(resnet_18_trained, 15, lr_tuner, train_from_layer=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(resnet_18_trained, '../models/trainedModels/owntrained/best-resnet-18.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained Model Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_model(filepath):\n",
    "    '''\n",
    "    Loads a trained model.\n",
    "    \n",
    "    :param filepath: Path to the trained model.\n",
    "    '''\n",
    "    loaded_model = torch.load(filepath, map_location=lambda storage, loc: storage)\n",
    "    return loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINED_MODELS_DIR = '/Volumes/TIMPP/TrainedModels'\n",
    "\n",
    "# current_model = load_model('/Volumes/TIMPP/TrainedModels/knmi-images/resnet-18/Resnet2/best-resnet-18_tuned.pt')\n",
    "current_model_trained = load_model(TRAINED_MODELS_DIR + '/knmi-highway/resnet-18/classifier/KNMIhighway_classifier.pth.tar')\n",
    "current_model_tuned = load_model(TRAINED_MODELS_DIR + '/knmi-highway/resnet-18/tuner/bestKNMIHighway_tuner.pth.tar')\n",
    "current_model_tuned = current_model_tuned['best_model']\n",
    "# type(current_model_tuned.load_state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Dataset and Dataloader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = KNMIDataset(test_features, test_targets, test_filepaths, transforms=data_transforms['validation'])\n",
    "test_dataloader = DataLoader(dataset=test_dataset, batch_size=len(test_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "current_model_train = load_model('../../../../Downloads/checkpoint123.pth.tar')\n",
    "current_model_train['best_epoch_avg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Test Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def test_model(model, dataloader):\n",
    "    \"\"\"\n",
    "    Tests a specified model on all the manually labeled highway camera\n",
    "    images. \n",
    "    \n",
    "    :param model: Trained model to evaluate\n",
    "    :param test_features: All test features as tensor\n",
    "    :param test_targets: All test labels as tensor\n",
    "    \"\"\"\n",
    "    \n",
    "    test_images, test_targets, idx, test_filepaths = next(iter(dataloader))\n",
    "    \n",
    "    # Loss criterion\n",
    "    criterion = nn.CrossEntropyLoss(reduce=False)\n",
    "    \n",
    "    # Wrap tensors\n",
    "    features = Variable(test_images)\n",
    "    targets = Variable(test_targets)\n",
    "    total = len(targets)\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # Feed test features into model\n",
    "    outputs = model(features)\n",
    "    \n",
    "    # Loss and optimization\n",
    "    loss = criterion(outputs, targets)\n",
    "    \n",
    "    # Get test predictions and number of correct predictions\n",
    "    _, predictions = torch.max(outputs.data, 1) \n",
    "    correct = torch.sum(predictions == targets.data)\n",
    "    \n",
    "    corrects = predictions == targets.data\n",
    "    \n",
    "    for i, cor in enumerate(corrects):\n",
    "        if predictions[i] == 0 and cor == 0:\n",
    "            print(test_filepaths[i])\n",
    "            print(test_targets[i])\n",
    "            img = test_features[i]\n",
    "            plt.imshow(img)\n",
    "            plt.show()\n",
    "             \n",
    "    image_indices = list(range(0, total))\n",
    "#     plot_images(loss, image_indices, test_filepaths, targets, predictions, phase='test')\n",
    "    \n",
    "    test_accuracy = correct / total * 100  \n",
    "    print('Test accuracy: {:.2f}%'.format(test_accuracy))\n",
    "    show_cm(list(targets.data), list(predictions))\n",
    "    \n",
    "test_model(current_model_tuned, test_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Data_Science_36",
   "language": "python",
   "name": "other-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
