{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torchsample\n",
    "import psycopg2\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "import csv\n",
    "import copy\n",
    "import statsmodels\n",
    "from functools import reduce \n",
    "from datetime import datetime\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data_utils\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "from torchsample import transforms as ts_transforms\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from geopy.distance import vincenty \n",
    "from scipy.ndimage import imread\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import preprocessing\n",
    "from math import sqrt\n",
    "from zipfile import ZipFile\n",
    "import pygrib\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from mlxtend.evaluate import confusion_matrix\n",
    "from mlxtend.plotting import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/helpers/DBconfig.csv', mode='r') as infile:\n",
    "    reader = csv.reader(infile)\n",
    "    config = {rows[0]:rows[1] for rows in reader}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connection to DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to database:\n",
      "host=fogdb.cmse2tqlcvsl.eu-west-1.rds.amazonaws.com port=9418 dbname=FOGDB user=KNMIFOG password=fog&mist123\n",
      "\n",
      "Tables in fog database:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('cameras',),\n",
       " ('day_phases',),\n",
       " ('images',),\n",
       " ('manual_annotations',),\n",
       " ('locations',),\n",
       " ('image_features',),\n",
       " ('meteo_features_stations',),\n",
       " ('test_table_images',),\n",
       " ('meteo_stations',),\n",
       " ('meteo_features_copy',)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Credentials fo AWS database\n",
    "dsn_database = config['database']           # e.g. \"compose\"\n",
    "dsn_hostname = \"fogdb.cmse2tqlcvsl.eu-west-1.rds.amazonaws.com\" # e.g.: \"aws-us-east-1-portal.4.dblayer.com\"\n",
    "dsn_port = \"9418\"                 # e.g. 11101 \n",
    "dsn_uid = config['\\ufeffusername']       # e.g. \"admin\"\n",
    "dsn_pwd = config ['password']     # e.g. \"xxx\"\n",
    "\n",
    "try:\n",
    "    conn_string = \"host=\"+dsn_hostname+\" port=\"+dsn_port+\" dbname=\"+dsn_database+\" user=\"+dsn_uid+\" password=\"+dsn_pwd\n",
    "    print(\"Connecting to database:\\n{}\\n\".format(conn_string))\n",
    "    conn=psycopg2.connect(conn_string)\n",
    "except:\n",
    "    print (\"\\nNo connection to the database could be established.\")\n",
    "\n",
    "# Create cursor\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Show database tables\n",
    "print('Tables in fog database:')\n",
    "cursor.execute(\"select relname from pg_class where relkind='r' and relname !~ '^(pg_|sql_)';\")\n",
    "cursor.fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acquire Target Data for Training (Subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "distance_df = pd.read_csv('../data/helpers/distanceKNMIStationsToLocations.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get All KNMI Images Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_path(filepath):\n",
    "    '''\n",
    "    Used for getting the right filepath.\n",
    "    '''\n",
    "    regex = re.compile(r'\\d[/].*$')\n",
    "    search = re.search(regex, filepath)\n",
    "    jpg_name = search.group(0)[2:]\n",
    "\n",
    "    # Second search for filepaths of highways\n",
    "    regex2 = re.compile(r'[A]\\d*-.*')\n",
    "    search2 = re.search(regex2, jpg_name)\n",
    "    \n",
    "    if search2 != None:\n",
    "        jpg_name = search2.group(0)\n",
    "        return str(jpg_name)\n",
    "    \n",
    "    return jpg_name\n",
    "\n",
    "cursor.execute(\"SELECT * FROM images WHERE day_phase = '1'\")\n",
    "\n",
    "# Fetch\n",
    "img_df = pd.DataFrame(cursor.fetchall(), columns=['img_id', 'camera_id', 'datetime', 'filepath', 'day_phase'])\n",
    "img_df['filepath'] = img_df['filepath'].apply(get_path, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bind Stations to Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fetch all the camera/location id pairs and put into df\n",
    "cursor.execute(\"SELECT * FROM cameras\")\n",
    "df_cameras = pd.DataFrame(cursor.fetchall(), columns=['camera_id', 'location_id', 'cam_description', 'camera_name'])\n",
    "\n",
    "# Merge image df with the cameras df and then with distance df\n",
    "merged_image_cameras = pd.merge(img_df, df_cameras, on='camera_id')\n",
    "merged_nearest = pd.merge(merged_image_cameras, distance_df, on='location_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Visibility and Meteo Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cursor.execute(\"SELECT * FROM meteo_features_copy\")\n",
    "\n",
    "df_meteo_features = pd.DataFrame(cursor.fetchall(), columns=['key','MeteoStationLocationID', 'datetime', \n",
    "                                                             'wind_speed', 'rel_humidity', 'air_temp', 'dew_point',\n",
    "                                                            'mor_vis'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Missing Meteorological Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Windspeed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 10000 wind speed values\n",
      "EELDE airport 2017-07-08 14:10:00\n",
      "EELDE airport 2017-07-08 14:20:00\n",
      "ROTTERDAM airport 2017-07-08 14:30:00\n",
      "EELDE airport 2017-07-08 14:30:00\n",
      "SCHIPHOL airport 2017-07-08 14:40:00\n",
      "ROTTERDAM airport 2017-07-08 14:40:00\n",
      "EELDE airport 2017-07-08 14:40:00\n",
      "SCHIPHOL airport 2017-07-08 14:50:00\n",
      "ROTTERDAM airport 2017-07-08 14:50:00\n",
      "EELDE airport 2017-07-08 14:50:00\n",
      "ROTTERDAM airport 2017-07-08 15:00:00\n",
      "EELDE airport 2017-07-08 15:00:00\n",
      "SCHIPHOL airport 2017-07-08 15:10:00\n",
      "ROTTERDAM airport 2017-07-08 15:10:00\n",
      "EELDE airport 2017-07-08 15:10:00\n",
      "SCHIPHOL airport 2017-07-08 15:20:00\n",
      "ROTTERDAM airport 2017-07-08 15:20:00\n",
      "EELDE airport 2017-07-08 15:20:00\n",
      "SCHIPHOL airport 2017-07-08 15:30:00\n",
      "ROTTERDAM airport 2017-07-08 15:30:00\n",
      "SCHIPHOL airport 2017-07-08 15:40:00\n",
      "ROTTERDAM airport 2017-07-08 15:40:00\n",
      "SCHIPHOL airport 2017-07-08 15:50:00\n",
      "ROTTERDAM airport 2017-07-08 15:50:00\n",
      "EELDE airport 2017-07-08 15:50:00\n",
      "ROTTERDAM airport 2017-07-08 16:00:00\n",
      "SCHIPHOL airport 2017-07-08 16:10:00\n",
      "ROTTERDAM airport 2017-07-08 16:10:00\n",
      "ROTTERDAM airport 2017-07-08 16:20:00\n",
      "EELDE airport 2017-07-08 16:20:00\n",
      "SCHIPHOL airport 2017-07-08 16:30:00\n",
      "ROTTERDAM airport 2017-07-08 16:30:00\n",
      "SCHIPHOL airport 2017-07-08 16:40:00\n",
      "ROTTERDAM airport 2017-07-08 16:40:00\n",
      "ROTTERDAM airport 2017-07-08 16:50:00\n",
      "ROTTERDAM airport 2017-07-08 17:00:00\n",
      "ROTTERDAM airport 2017-07-08 17:10:00\n",
      "EELDE airport 2017-07-08 17:10:00\n",
      "SCHIPHOL airport 2017-07-08 17:20:00\n",
      "EELDE airport 2017-07-08 17:30:00\n",
      "ROTTERDAM airport 2017-07-08 17:40:00\n",
      "EELDE airport 2017-07-08 17:40:00\n",
      "ROTTERDAM airport 2017-07-08 17:50:00\n",
      "EELDE airport 2017-07-08 17:50:00\n",
      "ROTTERDAM airport 2017-07-08 18:00:00\n",
      "EELDE airport 2017-07-08 18:00:00\n",
      "ROTTERDAM airport 2017-07-08 18:10:00\n",
      "SCHIPHOL airport 2017-07-08 18:20:00\n",
      "EELDE airport 2017-07-08 18:30:00\n",
      "ROTTERDAM airport 2017-07-08 18:50:00\n",
      "EELDE airport 2017-07-08 18:50:00\n",
      "ROTTERDAM airport 2017-07-08 19:00:00\n",
      "SCHIPHOL airport 2017-07-08 19:30:00\n",
      "ROTTERDAM airport 2017-07-08 19:30:00\n",
      "EELDE airport 2017-07-08 19:30:00\n",
      "ROTTERDAM airport 2017-07-08 19:40:00\n",
      "EELDE airport 2017-07-08 19:40:00\n",
      "ROTTERDAM airport 2017-07-08 19:50:00\n",
      "EELDE airport 2017-07-08 19:50:00\n",
      "SCHIPHOL airport 2017-07-08 22:10:00\n",
      "EELDE airport 2017-07-08 22:10:00\n",
      "ROTTERDAM airport 2017-07-08 22:30:00\n",
      "EELDE airport 2017-07-08 22:30:00\n",
      "ROTTERDAM airport 2017-07-13 18:10:00\n",
      "EELDE airport 2017-07-13 18:10:00\n",
      "Added 20000 wind speed values\n",
      "EELDE airport 2017-07-24 10:20:00\n",
      "ROTTERDAM airport 2017-08-11 09:20:00\n",
      "Added 30000 wind speed values\n",
      "SCHIPHOL airport 2017-08-23 05:00:00\n",
      "SCHIPHOL airport 2017-08-23 05:10:00\n",
      "SCHIPHOL airport 2017-08-23 06:30:00\n",
      "SCHIPHOL airport 2017-08-23 09:20:00\n",
      "Added 40000 wind speed values\n",
      "ROTTERDAM airport 2017-09-28 08:20:00\n",
      "ROTTERDAM airport 2017-09-28 08:30:00\n",
      "ROTTERDAM airport 2017-09-28 08:40:00\n",
      "ROTTERDAM airport 2017-09-28 08:50:00\n",
      "Added 50000 wind speed values\n",
      "ROTTERDAM airport 2017-10-18 05:50:00\n",
      "ROTTERDAM airport 2017-10-18 06:00:00\n",
      "ROTTERDAM airport 2017-10-18 06:10:00\n",
      "ROTTERDAM airport 2017-10-18 06:20:00\n",
      "ROTTERDAM airport 2017-10-18 06:30:00\n",
      "ROTTERDAM airport 2017-10-18 06:40:00\n",
      "ROTTERDAM airport 2017-10-18 06:50:00\n",
      "ROTTERDAM airport 2017-10-18 07:00:00\n",
      "ROTTERDAM airport 2017-10-18 07:10:00\n",
      "ROTTERDAM airport 2017-10-18 07:20:00\n",
      "ROTTERDAM airport 2017-10-18 07:30:00\n",
      "ROTTERDAM airport 2017-10-18 07:40:00\n",
      "ROTTERDAM airport 2017-10-18 07:50:00\n",
      "ROTTERDAM airport 2017-10-18 08:00:00\n",
      "ROTTERDAM airport 2017-10-18 08:10:00\n",
      "ROTTERDAM airport 2017-10-18 08:20:00\n",
      "ROTTERDAM airport 2017-10-18 08:30:00\n",
      "ROTTERDAM airport 2017-10-18 08:40:00\n",
      "ROTTERDAM airport 2017-10-18 08:50:00\n",
      "Added 60000 wind speed values\n",
      "Added 70000 wind speed values\n",
      "Added 80000 wind speed values\n",
      "Added 90000 wind speed values\n",
      "Added 100000 wind speed values\n"
     ]
    }
   ],
   "source": [
    "with open('../data/helpers/windspeed.csv', 'r') as file:\n",
    "    csv_read = csv.reader(file)\n",
    "    next(csv_read)\n",
    "    \n",
    "    c = 0\n",
    "    for row in csv_read:\n",
    "        if row[1] == '240_W_18Cm27':\n",
    "            location_name = 'SCHIPHOL airport'\n",
    "            meteo_id = 542\n",
    "        elif row[1] == '344_W_06t':\n",
    "            location_name = 'ROTTERDAM airport'\n",
    "            meteo_id = 536\n",
    "        elif row[1] == '380_W_04t':\n",
    "            location_name = 'EELDE airport'\n",
    "            meteo_id = 506\n",
    "            \n",
    "        date = row[0][:15].replace('_', '')\n",
    "        \n",
    "        if date[8:10] == '24':\n",
    "            date = date[:8] + '00' + date[10:]\n",
    "\n",
    "        datetime = datetime.strptime(date, '%Y%m%d%H%M%S')\n",
    "        indices = df_meteo_features[(df_meteo_features['datetime'] == datetime) & \n",
    "                          (df_meteo_features['MeteoStationLocationID'] == meteo_id)].index\n",
    "\n",
    "        for idx in indices:\n",
    "            try:\n",
    "                c += 1\n",
    "                df_meteo_features.at[idx, 'wind_speed'] = row[2]\n",
    "            except:\n",
    "                print(location_name, datetime)\n",
    "                \n",
    "            if c % 10000 == 0:\n",
    "                print('Added {} wind speed values'.format(c))         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temperature/Humidity Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 10000 temperature and humidity values\n",
      "Added 20000 temperature and humidity values\n",
      "Added 30000 temperature and humidity values\n"
     ]
    }
   ],
   "source": [
    "with open('../data/helpers/temp_humidity.csv', 'r') as file:\n",
    "    csv_read = csv.reader(file)\n",
    "    next(csv_read)\n",
    "    c = 0\n",
    "    for row in csv_read:\n",
    "        date = row[0][:15].replace('_', '')\n",
    "        air_temp = row[2]\n",
    "        rel_hum = row[4]\n",
    "        meteo_id = 542\n",
    "        \n",
    "        if date[8:10] == '24':\n",
    "            date = date[:8] + '00' + date[10:]\n",
    "            \n",
    "        datetime = datetime.strptime(date, '%Y%m%d%H%M%S')\n",
    "        \n",
    "        indices = df_meteo_features[(df_meteo_features['datetime'] == datetime) & \n",
    "                                    (df_meteo_features['MeteoStationLocationID'] == meteo_id)].index\n",
    "\n",
    "        for idx in indices:\n",
    "\n",
    "            try:\n",
    "                c += 1\n",
    "                df_meteo_features.at[idx, 'air_temp'] = air_temp\n",
    "                df_meteo_features.at[idx, 'rel_humidity'] = rel_hum\n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "            if c % 10000 == 0:\n",
    "                print('Added {} temperature and humidity values'.format(c))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"SELECT * FROM locations\")\n",
    "locations_df = pd.DataFrame(cursor.fetchall(), columns=['location_id', 'location_name', 'long', 'lat'])\n",
    "# locations_df[locations_df['location_name'] == 'A4-HM448-ID12002']\n",
    "# merged_nearest[merged_nearest['location_id'] == 355]\n",
    "\n",
    "test_loc_df = pd.merge(merged_nearest, locations_df, on='location_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A1-HM47-ID13970_20170630_0802.jpg\n",
      "A1-HM47-ID13970_20170630_0832.jpg\n",
      "A1-HM47-ID13970_20170630_0942.jpg\n",
      "A1-HM47-ID13970_20170630_1102.jpg\n",
      "A1-HM47-ID13970_20170630_1452.jpg\n",
      "A1-HM47-ID13970_20170630_1552.jpg\n",
      "A1-HM47-ID13970_20171202_1020.jpg\n",
      "A1-HM47-ID13970_20171222_0950.jpg\n",
      "A15-HM801-ID12047_20170801_0932.jpg\n",
      "A15-HM801-ID12047_20170803_1032.jpg\n",
      "A15-HM801-ID12047_20170805_1132.jpg\n",
      "A15-HM801-ID12047_20170807_1232.jpg\n",
      "A15-HM801-ID12047_20170809_1332.jpg\n",
      "A15-HM801-ID12047_20170811_1432.jpg\n",
      "A15-HM801-ID12047_20170813_1532.jpg\n",
      "A15-HM801-ID12047_20170815_1631.jpg\n",
      "A15-HM801-ID12047_20170817_1001.jpg\n",
      "A15-HM801-ID12047_20170819_1101.jpg\n",
      "A15-HM801-ID12047_20170821_1201.jpg\n",
      "A2-HM914-ID13472_20170901_0931.jpg\n",
      "A2-HM914-ID13472_20170903_1031.jpg\n",
      "A2-HM914-ID13472_20170905_1330.jpg\n",
      "A2-HM914-ID13472_20170907_1230.jpg\n",
      "A2-HM914-ID13472_20170908_1520.jpg\n",
      "A2-HM914-ID13472_20170911_1431.jpg\n",
      "A2-HM914-ID13472_20170913_1531.jpg\n",
      "A2-HM914-ID13472_20170915_1631.jpg\n",
      "A2-HM742-ID10906_20171001_0931.jpg\n",
      "A2-HM742-ID10906_20171002_1031.jpg\n",
      "A2-HM742-ID10906_20171003_1131.jpg\n",
      "A2-HM742-ID10906_20171006_1431.jpg\n",
      "A2-HM742-ID10906_20171008_1531.jpg\n",
      "A2-HM742-ID10906_20171010_1641.jpg\n",
      "A2-HM742-ID10906_20171202_0810.jpg\n",
      "A2-HM742-ID10906_20171222_0810.jpg\n",
      "A2-HM742-ID10906_20171222_0930.jpg\n",
      "A2-HM742-ID10906_20171222_1320.jpg\n",
      "A2-HM742-ID10906_20171222_1430.jpg\n",
      "A2-HM748-ID10907_20171101_0710.jpg\n",
      "A2-HM748-ID10907_20171102_1020.jpg\n",
      "A2-HM748-ID10907_20171103_0810.jpg\n",
      "A2-HM748-ID10907_20171104_1210.jpg\n",
      "A2-HM748-ID10907_20171106_1520.jpg\n",
      "A2-HM748-ID10907_20171107_0700.jpg\n",
      "A2-HM748-ID10907_20171109_1540.jpg\n",
      "A2-HM748-ID10907_20171128_1130.jpg\n",
      "A2-HM748-ID10907_20171202_1340.jpg\n",
      "A2-HM748-ID10907_20171202_1130.jpg\n",
      "A2-HM748-ID10907_20171202_1510.jpg\n",
      "A2-HM748-ID10907_20171203_1250.jpg\n",
      "A2-HM748-ID10907_20171129_1010.jpg\n",
      "A2-HM755-ID10910_20171201_0820.jpg\n",
      "A2-HM755-ID10910_20171203_0940.jpg\n",
      "A2-HM755-ID10910_20171205_1050.jpg\n",
      "A2-HM755-ID10910_20171211_1140.jpg\n",
      "A2-HM755-ID10910_20171212_1020.jpg\n",
      "A2-HM755-ID10910_20171220_0931.jpg\n",
      "A2-HM755-ID10910_20171221_0940.jpg\n",
      "A2-HM755-ID10910_20171223_1410.jpg\n",
      "A2-HM755-ID10910_20171231_1320.jpg\n",
      "A2-HM755-ID10910_20171202_1310.jpg\n",
      "A2-HM758-ID10911_20171201_1020.jpg\n",
      "A2-HM758-ID10911_20171201_0850.jpg\n",
      "A2-HM758-ID10911_20171202_1340.jpg\n",
      "A2-HM758-ID10911_20171202_1450.jpg\n",
      "A2-HM758-ID10911_20171203_1120.jpg\n",
      "A2-HM758-ID10911_20171204_1410.jpg\n",
      "A2-HM761-ID10912_20180201_0930.jpg\n",
      "A2-HM761-ID10912_20180210_0720.jpg\n",
      "A2-HM761-ID10912_20180210_1140.jpg\n",
      "A2-HM761-ID10912_20180211_1440.jpg\n",
      "A2-HM761-ID10912_20180211_1620.jpg\n",
      "A2-HM761-ID10912_20180212_0840.jpg\n",
      "A2-HM761-ID10912_20171201_0810.jpg\n",
      "A2-HM761-ID10912_20171202_1450.jpg\n",
      "A2-HM761-ID10912_20171222_0920.jpg\n",
      "A2-HM761-ID10912_20171222_0830.jpg\n",
      "A2-HM761-ID10912_20171222_1430.jpg\n",
      "A2-HM765-ID10913_20180301_1000.jpg\n",
      "A2-HM765-ID10913_20180301_1350.jpg\n",
      "A2-HM765-ID10913_20180307_1230.jpg\n",
      "A2-HM765-ID10913_20180307_1350.jpg\n",
      "A2-HM765-ID10913_20180308_0750.jpg\n",
      "A2-HM765-ID10913_20180308_1150.jpg\n",
      "A2-HM765-ID10913_20180308_1430.jpg\n",
      "A2-HM765-ID10913_20180309_1220.jpg\n",
      "A2-HM765-ID10913_20171202_1330.jpg\n",
      "A2-HM765-ID10913_20171221_1140.jpg\n",
      "A2-HM765-ID10913_20171222_1140.jpg\n",
      "A2-HM770-ID10914_20180401_0720.jpg\n",
      "A2-HM770-ID10914_20180401_1300.jpg\n",
      "A2-HM770-ID10914_20180403_0800.jpg\n",
      "A2-HM770-ID10914_20180403_1410.jpg\n",
      "A2-HM770-ID10914_20180404_1740.jpg\n",
      "A2-HM770-ID10914_20180405_0830.jpg\n",
      "A2-HM770-ID10914_20180405_1740.jpg\n",
      "A2-HM770-ID10914_20171201_0850.jpg\n",
      "A2-HM770-ID10914_20171202_1420.jpg\n",
      "A2-HM770-ID10914_20171222_1000.jpg\n",
      "A2-HM770-ID10914_20171222_0840.jpg\n",
      "A2-HM770-ID10914_20171222_1030.jpg\n",
      "A2-HM770-ID10914_20171222_1400.jpg\n",
      "A2-HM776-ID10915_20170701_0802.jpg\n",
      "A2-HM776-ID10915_20170702_0552.jpg\n",
      "A2-HM776-ID10915_20170705_1402.jpg\n",
      "A2-HM776-ID10915_20170706_0352.jpg\n",
      "A2-HM776-ID10915_20170706_1502.jpg\n",
      "A2-HM776-ID10915_20170708_1512.jpg\n",
      "A2-HM776-ID10915_20170711_1502.jpg\n",
      "A2-HM776-ID10915_20170714_0322.jpg\n",
      "A2-HM778-ID10916_20171201_1000.jpg\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-746fd5d3c190>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mtest_loc_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_loc_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'filepath'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/data_science_36/lib/python3.6/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, other, axis)\u001b[0m\n\u001b[1;32m    877\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 879\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mna_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    880\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m                 raise TypeError('Could not compare {typ} type with Series'\n",
      "\u001b[0;32m~/anaconda3/envs/data_science_36/lib/python3.6/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36mna_op\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_object_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 783\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_comp_method_OBJECT_ARRAY\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    784\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/data_science_36/lib/python3.6/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36m_comp_method_OBJECT_ARRAY\u001b[0;34m(op, x, y)\u001b[0m\n\u001b[1;32m    761\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvec_compare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalar_compare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    764\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with open('/Volumes/TIMKNMI/KNMIPictures/RWS/TestImages.txt') as filestream:\n",
    "    test_filenames = []\n",
    "    for row in filestream:\n",
    "        row = row.strip().split(',')\n",
    "        filename = row[0]\n",
    "        test_filenames.append(filename)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bind Images to Visibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loc_names = ['De Bilt (260_A_a)', 'Cabauw (348)', 'BEEK airport', 'EELDE airport', 'ROTTERDAM airport', 'SCHIPHOL airport']\n",
    "\n",
    "# Categorise images\n",
    "def ordinal_visibility(mor_vis):\n",
    "    if mor_vis > 1000:\n",
    "        return 0\n",
    "    elif mor_vis < 250:\n",
    "        return 2\n",
    "    elif mor_vis >= 250 and mor_vis <= 1000:\n",
    "        return 1\n",
    "    \n",
    "# Here the meteo features of closest meteo station are linked to every camera\n",
    "main_df = pd.merge(merged_nearest, df_meteo_features, on=['MeteoStationLocationID', 'datetime'])\n",
    "main_df['visibility'] = main_df['mor_vis'].apply(ordinal_visibility, 1)\n",
    "\n",
    "# Location to main df\n",
    "cursor.execute(\"SELECT * FROM locations\")\n",
    "locations_df = pd.DataFrame(cursor.fetchall(), columns=['location_id', 'location_name', 'long', 'lat'])\n",
    "main_df = pd.merge(locations_df, main_df, on='location_id')\n",
    "\n",
    "# Make sure that meteo variables above 7500m distance are nan. We want to fill these with kriging\n",
    "# main_df.loc[main_df.distanceInMeters > 7500, ['wind_speed', 'rel_humidity', 'air_temp', 'dew_point']] = np.nan\n",
    "main_df.loc[~main_df.location_name.isin(loc_names), ['wind_speed', 'rel_humidity', 'air_temp', 'dew_point']] = np.nan\n",
    "\n",
    "# # Distribution of labels\n",
    "main_df['visibility'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bind Location Names to Location IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "retain_columns = ['location_id', 'long', 'lat', 'location_name', 'camera_id', 'datetime', 'filepath', 'MeteoStationLocationID', \n",
    "                 'camera_name', 'meteo_station_id', 'meteo_station_name', 'distanceInMeters', 'wind_speed',\n",
    "                 'rel_humidity', 'air_temp', 'dew_point', 'mor_vis', 'visibility']\n",
    "\n",
    "main_df = main_df[retain_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change the File Path for Certain  Pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def change_filepaths(filepath):\n",
    "    '''\n",
    "    Changes the filepaths of old de Bilt images and of Cabauw images.\n",
    "    \n",
    "    :param filepath: Either a de Bilt or Cabauw image filepath.\n",
    "    '''\n",
    "    if 'nobackup/users/' in filepath:\n",
    "        return filepath[-29:]\n",
    "    elif 'CABAUW' in filepath:\n",
    "        return filepath[7:]\n",
    "    else:\n",
    "        return filepath\n",
    "    \n",
    "def change_cabauw(filepath):\n",
    "    if 'CABAUW' in filepath:\n",
    "        return filepath[7:]\n",
    "    else:\n",
    "        return filepath\n",
    "\n",
    "main_df['filepath'] = main_df['filepath'].apply(change_filepaths, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exclude NaN's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "main_df = main_df[main_df['visibility'].notnull()]\n",
    "main_df = main_df[main_df['rel_humidity'].notnull()]\n",
    "main_df = main_df[main_df['wind_speed'].notnull()]\n",
    "main_df= main_df[main_df['air_temp'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_zipfile_path(date):\n",
    "\n",
    "    original_hour = date[-2:]\n",
    "\n",
    "    # Change the hour for getting right shit\n",
    "    if int(original_hour) < 7:\n",
    "        hour = '00'\n",
    "    elif 6 < int(original_hour) < 13:\n",
    "        hour = '06'\n",
    "    elif 12 < int(original_hour) < 19:\n",
    "        hour = '12'\n",
    "    elif 18 < int(original_hour) < 24:\n",
    "        hour = '18'\n",
    "\n",
    "    zip_date = date[:-2]+hour\n",
    "\n",
    "    # Get path to zipfile\n",
    "    zipfile_name = 'HARM_{}_P1.zip'.format(zip_date)\n",
    "    zip_path = '/Volumes/externe schijf/timWeatherModel/{}'.format(zipfile_name)\n",
    "    \n",
    "    return zip_path, zip_date\n",
    "\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "def open_grib(zip_path, zip_date, date):\n",
    "    # Create temporary directory\n",
    "    tDir = tempfile.mkdtemp('Harmonies')\n",
    "    \n",
    "    with TemporaryDirectory() as tmp_dir:\n",
    "        \n",
    "        # Open zipfile and get grib filepath\n",
    "        archive = ZipFile(zip_path)\n",
    "        grib_name = 'HA36_P1_{}00_0{}00_GB'.format(zip_date, date[-2:])\n",
    "\n",
    "        # Extract right grib file from zip and bring to temporary directory\n",
    "        extract_zip = archive.extract(grib_name, tmp_dir)\n",
    "        gribfile = pygrib.open('{}/{}'.format(tmp_dir, grib_name))\n",
    "    \n",
    "    return gribfile\n",
    "\n",
    "def get_latlon_idx(lats, longs, obs_lat, obs_lon):\n",
    "    # Get the absolute distances between all grid points and observation point\n",
    "    abslat = np.abs(lats - obs_lat)\n",
    "    abslon = np.abs(lons - obs_lon)\n",
    "\n",
    "    # Get the absolute distance for \n",
    "    lat_index = np.argmin(abslat)\n",
    "    lon_index = np.argmin(abslon)\n",
    "\n",
    "    c = np.maximum(abslon, abslat)\n",
    "    latlon_idx = np.argmin(c)\n",
    "    \n",
    "    return latlon_idx\n",
    "\n",
    "def calculate_dewpoint(RH, T):\n",
    "    '''\n",
    "    Calculates dewpoint give relative humidity and temperature. 0.08 is added at the end, because otherwise\n",
    "    calculations would be inconsistent with dataframe values.\n",
    "    '''\n",
    "    RH = RH * 100\n",
    "    DP = 243.04*(np.log(RH/100)+((17.625*T)/(243.04+T)))/(17.625-np.log(RH/100)-((17.625*T)/(243.04+T))) + 0.08\n",
    "    return DP\n",
    "\n",
    "def get_meteo_grids(grib):\n",
    "    \n",
    "\n",
    "    # Get variable values for date\n",
    "    temps = grib.select(name='2 metre temperature')[0]\n",
    "    rel_hums = grib.select(name='Relative humidity')[0]\n",
    "\n",
    "    # Calculate the wind speed\n",
    "    windcomponent_U = grib.select(name='10 metre U wind component')[0].values\n",
    "    windcomponent_V = grib.select(name='10 metre V wind component')[0].values\n",
    "    windspeeds = np.sqrt(windcomponent_U ** 2 + windcomponent_V **2)\n",
    "\n",
    "    # Latitudes and longitudes are the same for every variable, because same grid is used\n",
    "    lats, lons = temps.latlons()\n",
    "\n",
    "    temps, rel_hums = temps.values, rel_hums.values\n",
    "    \n",
    "    return lats, lons, temps, rel_hums, windspeeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique_dates = main_df['datetime'].unique()\n",
    "num_dates = len(unique_dates)\n",
    "\n",
    "for c, one_date in enumerate(unique_dates):\n",
    "\n",
    "    # Get df for unique date and extract date\n",
    "    date_df = main_df[main_df['datetime'] == one_date]\n",
    "    \n",
    "    # Convert date to string for getting zipfile\n",
    "    t = pd.to_datetime(str(one_date)) \n",
    "    timestring = t.strftime('%Y%m%d%H')\n",
    "    \n",
    "    # Get the zipfile path and the right date for getting gribfile in ZIP\n",
    "    zip_path, zip_date = get_zipfile_path(timestring)\n",
    "    \n",
    "    try:\n",
    "        grib = open_grib(zip_path, zip_date, timestring)\n",
    "\n",
    "        # Obtain grid point positions and meteo variables\n",
    "        lats, lons, temps, rel_hums, windspeeds = get_meteo_grids(grib)\n",
    "\n",
    "        for idx, row in date_df.iterrows():\n",
    "\n",
    "            # Get the closest lat/lon index of grid\n",
    "            obs_lat, obs_lon = row['lat'], row['long']\n",
    "            closest_idx = get_latlon_idx(lats, lons, obs_lat, obs_lon)\n",
    "\n",
    "            # Get the variable values\n",
    "            temp, rel_hum, windspeed = temps.flat[closest_idx], rel_hums.flat[closest_idx], windspeeds.flat[closest_idx]\n",
    "            temp = float(temp) - 272.15\n",
    "\n",
    "            dew_point = calculate_dewpoint(rel_hum, temp)\n",
    "\n",
    "            if np.isnan(row['air_temp']):\n",
    "                main_df.at[idx, 'air_temp'] = temp\n",
    "            if np.isnan(row['rel_humidity']):\n",
    "                main_df.at[idx, 'rel_humidity'] = rel_hum\n",
    "            if np.isnan(row['wind_speed']):\n",
    "                main_df.at[idx, 'wind_speed'] = windspeed\n",
    "            if np.isnan(row['dew_point']):\n",
    "                main_df.at[idx, 'dew_point'] = dew_point\n",
    "\n",
    "    except:\n",
    "        print('Grib-file unavailable at index: {}, for date: {}'.format(c, timestring))\n",
    "        continue\n",
    "\n",
    "    if c % 100 == 0:\n",
    "        print('Iterated over {} of {} unique dates'.format(c, num_dates))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickle or Unpickle the Main Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = input('do you want to pickle a new main_df dataframe? (y/n)')\n",
    "\n",
    "if q1 == 'y':\n",
    "    main_df.to_pickle('../data/semi-processed/meteo_standardize_df')\n",
    "    print('succesfully pickled DF')\n",
    "else:\n",
    "    q2 = input('do you want to unpickle the current saved dataframe? (y/n)')\n",
    "    if q2 == 'y':\n",
    "        main_df = pd.read_pickle('../data/semi-processed/filled_harmonie')\n",
    "        print('succesfully unpickled DF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "KNMI_names = ['De Bilt (260_A_a)', 'Cabauw (348)', 'BEEK airport', 'EELDE airport', 'ROTTERDAM airport', 'SCHIPHOL airport']\n",
    "meteo_variables = ['wind_speed', 'air_temp', 'rel_humidity', 'dew_point']\n",
    "\n",
    "def distribution_meteo_variables(main_df):\n",
    "    clip = {'wind_speed': (0, 12), 'air_temp': (-1000,1000), 'rel_humidity': (90, 120), 'dew_point':(-1000, 1000)}\n",
    "    \n",
    "    plot_df = main_df[main_df['location_name'].isin(KNMI_names)]\n",
    "    fig = plt.figure(figsize=(15, 10))\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    # Boxplot for every variable\n",
    "    for i, variable in enumerate(meteo_variables):\n",
    "        \n",
    "        if variable == 'rel_humidity':\n",
    "            plot_df = plot_df[plot_df[variable] > 95]\n",
    "    \n",
    "        target_0 = plot_df.loc[plot_df['visibility'] == 0]\n",
    "        target_1 = plot_df.loc[plot_df['visibility'] == 1]\n",
    "        target_2 = plot_df.loc[plot_df['visibility'] == 2]\n",
    "        \n",
    "        # Make-up\n",
    "        fig.add_subplot(2, 2, i + 1)\n",
    "        plt.title('Density plot of {}'.format(variable))\n",
    "        plt.ylabel('Density')\n",
    "        \n",
    "        sns.distplot(target_0[variable], hist=False, kde=True, kde_kws = {'linewidth':3, 'clip': clip[variable], 'shade':True}, label='no fog')\n",
    "        sns.distplot(target_1[variable], hist=False, kde=True, kde_kws = {'linewidth':3,'clip': clip[variable], 'shade':True}, label='light fog')\n",
    "        sns.distplot(target_2[variable], hist=False, kde=True, kde_kws = {'linewidth':3,'clip': clip[variable], 'shade':True},label='dense fog')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "boxplot_meteo_variables(main_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Close Database Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 100\n",
    "CHANNELS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Manipulate Test Data (Manually Labeled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELED_DIR = '/Volumes/TIMKNMI/KNMIPictures/RWS/'\n",
    "\n",
    "# Used for storing filename:label pairs\n",
    "image_list_test = []\n",
    "target_list_test = []\n",
    "filepath_list_test = []\n",
    "meteo_list_test = []\n",
    "\n",
    "# This opens all labeled files and finds the corresponding pictures and labels\n",
    "with open(LABELED_DIR + 'TestImages.txt') as filestream:\n",
    "    c = 0\n",
    "    for row in filestream:\n",
    "        row = row.strip().split(',')\n",
    "        filename = row[0]\n",
    "        label = row[1]\n",
    "        \n",
    "        datapoint_df = main_df[main_df['filepath'] == filename]\n",
    "        meteo = datapoint_df[['wind_speed', 'rel_humidity', 'air_temp', 'dew_point']]\n",
    "#         print(filename)\n",
    "        if len(np.asarray(meteo)) > 0:\n",
    "            print(meteo)\n",
    "            # Regex necessary elements\n",
    "            highway = re.search(r'A\\d*', filename).group(0)\n",
    "            ID = re.search(r'ID\\d*', filename).group(0)\n",
    "            HM = re.search(r'HM\\d*', filename).group(0)\n",
    "            year_month = re.search(r'_\\d*_', filename).group(0)[1:7]\n",
    "\n",
    "            path = '{}{}/{}/{}/{}/{}'.format(LABELED_DIR, highway, HM, ID, year_month, filename)\n",
    "\n",
    "            img = Image.open(path)\n",
    "            img = img.resize((IMG_SIZE, IMG_SIZE))\n",
    "\n",
    "            image_list_test.append(np.asarray(img))\n",
    "            target_list_test.append(np.asarray(label))\n",
    "            filepath_list_test.append(np.asarray(path))\n",
    "            meteo_list_test.append(np.asarray(meteo))\n",
    "\n",
    "            img.close()\n",
    "        else:\n",
    "            print(filename)\n",
    "            \n",
    "\n",
    "test_features, test_targets, test_filepaths, test_meteo = np.asarray(image_list_test), np.asarray(target_list_test).astype(int), np.asarray(filepath_list_test), np.asarray(meteo_list_test)\n",
    "# test_meteo = test_meteo.reshape(len(test_meteo), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "light = np.where(test_targets == 1)[0]\n",
    "dense = np.where(test_targets == 2)[0]\n",
    "\n",
    "for idx in light:\n",
    "    print(test_filepaths[idx])\n",
    "    img = test_features[idx]\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load KNMI Train Data to Numpy Arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Numpy Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "KNMI_DIR = '/Volumes/TIMPP/KNMIPictures/'\n",
    "\n",
    "image_list = []\n",
    "target_list = []\n",
    "filepath_list = []\n",
    "meteo_list = []\n",
    "\n",
    "KNMI_names = ['De Bilt (260_A_a)', 'Cabauw (348)', 'BEEK airport', 'EELDE airport', 'ROTTERDAM airport', 'SCHIPHOL airport']\n",
    "KNMI_df = main_df[main_df['location_name'].isin(KNMI_names)]\n",
    "\n",
    "for c, (index, row) in enumerate(KNMI_df.iterrows()):\n",
    "    folder = row['filepath'].split('-')[0]\n",
    "    folder = folder.split('_')[0]\n",
    "    camera = row['camera_name']\n",
    "    year_month = row['datetime'].strftime(\"%Y%m\")  \n",
    "    file = row['filepath']\n",
    "    meteo = row[['wind_speed', 'rel_humidity', 'air_temp', 'dew_point']]\n",
    "    \n",
    "    filepath = '/Volumes/TIMPP/KNMIPictures/{}/{}/{}/{}'.format(folder, camera, year_month, file)\n",
    "\n",
    "    try:\n",
    "        img = Image.open(filepath)\n",
    "        img = img.resize((IMG_SIZE, IMG_SIZE))\n",
    "        image_list.append(np.asarray(img))\n",
    "        img.close()\n",
    "\n",
    "        # Get target, append to target list\n",
    "        target = main_df[main_df['filepath'] == file]['visibility']\n",
    "        target_list.append(target.iloc[0])\n",
    "        filepath_list.append(filepath)\n",
    "        meteo_list.append(meteo)\n",
    "\n",
    "    # Catch erroneous files and continue if caught\n",
    "    except:\n",
    "        print('Could not load image: {}'.format(file))\n",
    "        continue\n",
    "    \n",
    "    # Print number of loaded images every 500 iterations\n",
    "    if c % 500 == 0:\n",
    "        print('Number of df rows iterated: {} of {}'.format(c, len(KNMI_df)))\n",
    "\n",
    "KNMI_images, KNMI_targets, KNMI_filepaths, KNMI_meteo = np.asarray(image_list), np.asarray(target_list), np.asarray(filepath_list), np.asarray(meteo_list)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ommit Labels and Change Labels KNMI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get indices to delete from KNMI numpy arrays\n",
    "del_knmi_idx = [i for i, v in enumerate(KNMI_filepaths) if 'BSRN-1' in v or 'Meetterrein_201606' in v or 'Meetterrein_201607' in v\n",
    "            or 'Meetterrein_201608' in v]\n",
    "\n",
    "# Ommit bad images from numpy arrays\n",
    "KNMI_targets = np.delete(KNMI_targets, del_knmi_idx, 0)\n",
    "KNMI_images = np.delete(KNMI_images, del_knmi_idx, 0)\n",
    "KNMI_filepaths = np.delete(KNMI_filepaths, del_knmi_idx, 0)\n",
    "KNMI_meteo = np.delete(KNMI_meteo, del_knmi_idx, 0)\n",
    "\n",
    "# Change targets of wrongly labeled images\n",
    "with open('../data/helpers/knmichangelabels') as file:\n",
    "    for row in file:\n",
    "        path, target = row.split(',')\n",
    "        path_idx = np.where(KNMI_filepaths == path.strip(\"'\"))\n",
    "        KNMI_targets[path_idx] = target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "KNMI_targets = np.load('../data/processed/KNMI_train_targets.npy')\n",
    "KNMI_images = np.load('../data/processed/KNMI_train_images.npy')\n",
    "KNMI_filepaths = np.load('../data/processed/KNMI_train_filepaths.npy')\n",
    "KNMI_meteo = np.load('../data/processed/KNMI_train_meteo.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Highway Train Data to Numpy Arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Highway DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "schiphol_df = main_df[main_df['MeteoStationLocationID'] == 542]\n",
    "schiphol_highways = schiphol_df[schiphol_df['location_name']!= 'SCHIPHOL airport']\n",
    "eelde_df = main_df[main_df['MeteoStationLocationID'] == 506]\n",
    "eelde_highways = eelde_df[eelde_df['location_name'] != 'EELDE airport']\n",
    "highway_df = pd.concat([eelde_highways, schiphol_highways])\n",
    "highway_df = highway_df[highway_df['distanceInMeters'] < 7500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform Data to Numpy Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_list = []\n",
    "target_list = []\n",
    "filepath_list = []\n",
    "meteo_list = []\n",
    "count = 1\n",
    "\n",
    "for index, row in highway_df.iterrows():\n",
    "    location = row['location_name'].split('-')\n",
    "    year_month = row['datetime'].strftime(\"%Y%m\")\n",
    "    file = row['filepath']\n",
    "    meteo = row[['wind_speed', 'rel_humidity', 'air_temp', 'dew_point']]\n",
    "    filepath = '/Volumes/TIMKNMI/KNMIPictures/UnlabeledRWS/{}/{}/{}/{}/{}'.format(location[0], location[1], location[2],\n",
    "                                                                                 year_month, file)\n",
    "    try:\n",
    "        img = Image.open(filepath)\n",
    "        img = img.resize((IMG_SIZE, IMG_SIZE))\n",
    "        image_list.append(np.asarray(img))\n",
    "        img.close()\n",
    "\n",
    "        # Get target, append to target list\n",
    "        target = main_df[main_df['filepath'] == file]['visibility']\n",
    "        target_list.append(target.iloc[0])\n",
    "        filepath_list.append(filepath)\n",
    "        meteo_list.append(meteo)\n",
    "        count += 1\n",
    "\n",
    "        # Print number of loaded images every 500 iterations\n",
    "        if count % 500 == 0:\n",
    "            print('Number of images loaded: {}'.format(count))\n",
    "\n",
    "    #Catch erroneous files and continue if caught\n",
    "    except:\n",
    "        print('Could not load image: {}'.format(file))\n",
    "        continue\n",
    "    \n",
    "highway_images, highway_targets, highway_filepaths, highway_meteo = np.asarray(image_list), np.asarray(target_list), np.asarray(filepath_list), np.asarray(meteo_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "highway_images = np.load('/Volumes/TIMPP/UnusedKNMI/numpyfiles/meteo/valtestinterpolated/highway/highway_images_allIDW.npy')\n",
    "highway_targets = np.load('/Volumes/TIMPP/UnusedKNMI/numpyfiles/meteo/valtestinterpolated/highway/highway_targets_allIDW.npy')\n",
    "highway_filepaths = np.load('/Volumes/TIMPP/UnusedKNMI/numpyfiles/meteo/valtestinterpolated/highway/highway_filepaths_allIDW.npy')\n",
    "highway_meteo = np.load('/Volumes/TIMPP/UnusedKNMI/numpyfiles/meteo/valtestinterpolated/highway/highway_meteo_allIDW.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ommit Labels and Change Labels Highway"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../data/helpers/trainhighwaylabels') as file:\n",
    "    for row in file:\n",
    "        path, target = row.split(',')\n",
    "        path_idx = np.where(highway_filepaths == path.strip(\"'\"))\n",
    "        highway_targets[path_idx] = target\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "meteo_df = main_df[['wind_speed', 'rel_humidity', 'air_temp', 'dew_point']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ommit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../data/helpers/ommitancehighway') as file:\n",
    "    indices = []\n",
    "    for row in file:\n",
    "        row = \" \".join(row.split())\n",
    "        path = row.strip(\"'\")\n",
    "        path_idx = np.where(highway_filepaths == row.strip(\"'\"))\n",
    "        indices.append(path_idx[0][0])\n",
    "        \n",
    "highway_targets = np.delete(highway_targets, indices, 0)\n",
    "highway_images = np.delete(highway_images, indices, 0)\n",
    "highway_filepaths = np.delete(highway_filepaths, indices, 0)\n",
    "highway_meteo = np.delete(highway_meteo, indices, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('../data/processed/highway_targets_allIDW.npy', highway_targets)\n",
    "np.save('../data/processed/highway_images_allIDW.npy', highway_images)\n",
    "np.save('../data/processed/highway_filepaths_allIDW.npy', highway_filepaths)\n",
    "np.save('../data/processed/highway_meteo_allIDW.npy', highway_meteo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = np.load('/Volumes/TIMPP/UnusedKNMI/numpyfiles/meteo/valtestinterpolated/highway/highway_images_allIDW.npy')\n",
    "highway_targets = np.load('/Volumes/TIMPP/UnusedKNMI/numpyfiles/meteo/valtestinterpolated/highway/highway_targets_allIDW.npy')\n",
    "highway_filepaths = np.load('/Volumes/TIMPP/UnusedKNMI/numpyfiles/meteo/valtestinterpolated/highway/highway_filepaths_allIDW.npy')\n",
    "highway_meteo = np.load('/Volumes/TIMPP/UnusedKNMI/numpyfiles/meteo/valtestinterpolated/highway/highway_meteo_allIDW.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "concat = np.concatenate([meteo_df, test_meteo])\n",
    "standardize_df = standardize_df.dropna(axis=0)\n",
    "std_scale = preprocessing.StandardScaler().fit(standardize_df[[0, 1, 2, 3]])\n",
    "highway_meteo = std_scale.transform(standardize_df[[0, 1, 2, 3]])\n",
    "test_meteo = highway_meteo[-len(test_meteo):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize Meteo Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "standardize_df = pd.DataFrame(highway_meteo, columns=[0, 1, 2, 3])\n",
    "standardize_df = standardize_df.dropna(axis=0)\n",
    "std_scale = preprocessing.StandardScaler().fit(standardize_df[[0, 1, 2, 3]])\n",
    "highway_meteo = std_scale.transform(standardize_df[[0, 1, 2, 3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Splits\n",
    "Only highway data is split. KNMI data will be used for training as a whole and tested on the highway validation dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Data\n",
    "5 of the labelled highway cameras will be used for validation of both the highway train dataset and KNMI train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "camera_list = ['A28/HM1893', 'A4/HM103', 'A5/HM86', 'A9/HM302', 'A28/HM1966']\n",
    "\n",
    "val_idx = [i for camera in camera_list for i, v in enumerate(highway_filepaths) if camera in v]\n",
    "X_validation, y_validation = highway_images[val_idx], highway_targets[val_idx].astype(int)\n",
    "paths_validation, meteo_validation = highway_filepaths[val_idx], highway_meteo[val_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Highway Train Data\n",
    "Highway images that are used for the validation data are deleted. What remains is the highway train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_highway = np.delete(highway_images, val_idx, 0)\n",
    "y_train_highway = np.delete(highway_targets, val_idx, 0).astype(int)\n",
    "paths_train_highway = np.delete(highway_filepaths, val_idx, 0)\n",
    "meteo_train_highway = np.delete(highway_meteo, val_idx, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the class proportions\n",
    "class_counts = np.bincount(y_train_highway.astype(int))\n",
    "total = len(y_train_highway)\n",
    "proportion_0 = class_counts[0] / total\n",
    "proportion_1 = class_counts[1] / total\n",
    "proportion_2 = class_counts[2] / total\n",
    "\n",
    "print('Class percentages:\\nNo fog: {:.2f}%\\nFog: {:.2f}%\\nDense fog: {:.2f}%'.format(proportion_0 * 100,\n",
    "                                                                              proportion_1 * 100, proportion_2 * 100))\n",
    "print(class_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Numpy Arrays\n",
    "Write the images, targets and filepaths to numpy arrays. These will be used for training and testing of the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNMI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('../data/processed/knmi_train_images_no_change.npy', KNMI_images)\n",
    "np.save('../data/processed/knmi_train_targets_no_change.npy', KNMI_targets)\n",
    "np.save('../data/processed/knmi_train_filepaths_no_change.npy', KNMI_filepaths)\n",
    "np.save('../data/processed/knmi_train_meteo_no_change.npy', KNMI_meteo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Highway Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('../data/processed/highway_train_images1.npy', X_train_highway)\n",
    "np.save('../data/processed/highway_train_targets1.npy', y_train_highway)\n",
    "np.save('../data/processed/highway_train_filepaths1.npy', paths_train_highway)\n",
    "np.save('../data/processed/highway_train_meteo1.npy', meteo_train_highway)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Highway Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('../data/processed/highway_val_images.npy', X_validation)\n",
    "np.save('../data/processed/highway_val_targets.npy', y_validation)\n",
    "np.save('../data/processed/highway_val_filepaths.npy', paths_validation)\n",
    "np.save('../data/processed/highway_val_meteo.npy', meteo_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Highway Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('../data/processed/test_images_meteo.npy', test_features)\n",
    "np.save('../data/processed/test_targets_meteo.npy', test_targets)\n",
    "np.save('../data/processed/test_filepaths_meteo.npy', test_filepaths)\n",
    "np.save('../data/processed/test_meteo_meteo.npy', test_meteo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA\n",
    "Optional to do a PCA for any of the datasets. It's advisable to only use a part of the no fog class data, because otherwise it's too computationally heavy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "highway_images = np.load('/Volumes/TIMPP/UnusedKNMI/numpyfiles/meteo/testinterpolated/highway/change/highway_images.npy')\n",
    "highway_targets = np.load('/Volumes/TIMPP/UnusedKNMI/numpyfiles/meteo/testinterpolated/highway/change/highway_targets.npy')\n",
    "highway_filepaths = np.load('/Volumes/TIMPP/UnusedKNMI/numpyfiles/meteo/testinterpolated/highway/change/highway_filepaths.npy')\n",
    "highway_meteo = np.load('/Volumes/TIMPP/UnusedKNMI/numpyfiles/meteo/testinterpolated/highway/change/highway_meteo.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nofog_idx = np.where(highway_targets == 0)\n",
    "light_idx = np.where(highway_targets == 1)\n",
    "dense_idx = np.where(highway_targets == 2)\n",
    "nofog_idx = np.random.choice(nofog_idx[0], 10000)\n",
    "\n",
    "# Select images and targets\n",
    "nofog_img, nofog_target = highway_images[nofog_idx], highway_targets[nofog_idx]\n",
    "light_img, light_target = highway_images[light_idx], highway_targets[light_idx]\n",
    "dense_img, dense_target = highway_images[dense_idx], highway_targets[dense_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PCA_img = np.concatenate((nofog_img, light_img, dense_img))\n",
    "PCA_targets = np.concatenate((nofog_target, light_target, dense_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "def run_PCA(X_data, y_data):\n",
    "    '''\n",
    "    Gets 2-dimensional principal components for pixel data\n",
    "    \n",
    "    :param X_data: Pixel values of images\n",
    "    :param y_data: Corresponding target values of images\n",
    "    '''\n",
    "    \n",
    "    # Define and transform data\n",
    "    pca = PCA(n_components=2)\n",
    "    principal_components= pca.fit_transform(X_data.flatten().reshape(len(X_data), IMG_SIZE * IMG_SIZE * CHANNELS))\n",
    "    \n",
    "    # Create DF of PCA's and targets \n",
    "    PCA_df = pd.DataFrame(data=principal_components, columns=['PCA1', 'PCA2'])\n",
    "    target_df = pd.DataFrame(data=y_data, columns=['target'])\n",
    "    PCA_df = pd.concat([PCA_df, target_df['target']], axis=1)\n",
    "    \n",
    "    return PCA_df\n",
    "\n",
    "def run_TSNE(X_data, y_data):\n",
    "    tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n",
    "    p_c = tsne.fit_transform(X_data.flatten().reshape(len(X_data), IMG_SIZE * IMG_SIZE * CHANNELS))\n",
    "    \n",
    "    tsne_df = pd.DataFrame(data=p_c, columns=['PCA1', 'PCA2'])\n",
    "    target_df = pd.DataFrame(data=y_data, columns=['target'])\n",
    "    tsne_df = pd.concat([tsne_df, target_df['target']], axis=1)\n",
    "    \n",
    "    return tsne_df\n",
    "\n",
    "def plot_PCA(PCA_df):\n",
    "    '''\n",
    "    This plots a PCA dataframe created with the run_PCA function\n",
    "    \n",
    "    :param PCA_df: PCA dataframe\n",
    "    \n",
    "    '''\n",
    "    # Create figure\n",
    "    fig = plt.figure(figsize= (8,8))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_xlabel(\"PCA1\")\n",
    "    ax.set_ylabel(\"PCA2\")\n",
    "    targets = [0, 1, 2]\n",
    "    class_names= ['Clear', 'Light Fog', 'Dense Fog']\n",
    "    colors = ['r', 'g', 'b']\n",
    "                                            \n",
    "    for target, color in zip(targets, colors):\n",
    "        indicesToKeep = PCA_df['target'] == target\n",
    "        ax.scatter(PCA_df.loc[indicesToKeep, 'PCA1'],\n",
    "                   PCA_df.loc[indicesToKeep, 'PCA2'],\n",
    "                   c = color,\n",
    "                   s = 50)\n",
    "\n",
    "        ax.legend(class_names)\n",
    "        ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example PCA Analysis\n",
    "PCA on a part of the highway dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run PCA\n",
    "PCA_df = run_TSNE(PCA_img, PCA_targets)\n",
    "plot_PCA(PCA_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Data_Science_36",
   "language": "python",
   "name": "other-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
